{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial.distance import cdist\n",
    "import ot\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here goes explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularized_OT_classification(S, T, rege, y_target):\n",
    "    # Initialize the scaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Z-score normalization \n",
    "    S_normalized = scaler.fit_transform(S) \n",
    "    T_normalized = scaler.fit_transform(T)\n",
    "\n",
    "    # Initial PCA\n",
    "    pca_S = PCA().fit(S_normalized)  # Without specifying n_components\n",
    "    pca_T = PCA().fit(T_normalized)\n",
    "\n",
    "    # Explained variance ratio\n",
    "    explained_variance_S = pca_S.explained_variance_ratio_\n",
    "    explained_variance_T = pca_T.explained_variance_ratio_\n",
    "\n",
    "    # Cumulative explained variance\n",
    "    cumulative_variance_S = explained_variance_S.cumsum()\n",
    "    cumulative_variance_T = explained_variance_T.cumsum()\n",
    "\n",
    "    # Calculation of ns and nt (number of components) to capture 90% of the variance\n",
    "    ds = next(i for i, total in enumerate(cumulative_variance_S) if total >= 0.90) + 1\n",
    "    dt = next(i for i, total in enumerate(cumulative_variance_T) if total >= 0.90) + 1\n",
    "\n",
    "    # Select the minimum number of components to ensure equal dimensionality\n",
    "    d = min(ds, dt)\n",
    "\n",
    "    # PCA with optimal d value\n",
    "    pca_S_optimal = PCA(n_components=d)  # Use the minimum value for d\n",
    "    Xs = pca_S_optimal.fit_transform(S_normalized)\n",
    "\n",
    "    pca_T_optimal = PCA(n_components=d)  # Use the same value for d\n",
    "    Xt = pca_T_optimal.fit_transform(T_normalized)\n",
    "\n",
    "    # Uniform vectors with equal size to ns and nt (same value per array, both values less than 1)\n",
    "    a = np.ones(Xs.shape[0]) / Xs.shape[0]  \n",
    "    b = np.ones(Xt.shape[0]) / Xt.shape[0]\n",
    "\n",
    "    # Calculate the cost matrix using Euclidean distance\n",
    "    M = cdist(Xs, Xt, metric='euclidean')  # Calculate distances between samples in Xs and Xt\n",
    "\n",
    "    # Normalize the cost matrix by the maximum value of M\n",
    "    M_normalized = M / M.max()\n",
    "\n",
    "    # Fit source to target using the Sinkhorn algorithm\n",
    "    gamma = ot.sinkhorn(a, b, M_normalized, rege)\n",
    "\n",
    "    # Transport points from S to T\n",
    "    Sa = np.dot(gamma.T, Xs)  # Shape of transported points Sa\n",
    "\n",
    "    # Fit a 1-NN classifier on transported source points Sa and their corresponding labels\n",
    "    knn = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "    # Fit the classifier on transported source points Sa\n",
    "    knn.fit(Sa, y_target)  # Use the target labels passed as a parameter\n",
    "\n",
    "    # Step 3: Make predictions on the target points T\n",
    "    y_pred = knn.predict(Xt)\n",
    "\n",
    "    # Calculate accuracy of Sa over y_target\n",
    "    accuracy = accuracy_score(y_target, y_pred)\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Aplication on the Office/Caltech dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on target observations: 0.8726114649681529\n"
     ]
    }
   ],
   "source": [
    "# Dataset with 4096 features (CaffeNet4096)\n",
    "caffe_dir = 'CaffeNet4096/'\n",
    "data_webcam = loadmat(os.path.join(caffe_dir, 'webcam.mat'))\n",
    "X_webcam = data_webcam['fts']\n",
    "y_webcam = data_webcam['labels'].flatten()\n",
    "\n",
    "data_dslr = loadmat(os.path.join(caffe_dir, 'dslr.mat'))\n",
    "X_dslr = data_dslr['fts']\n",
    "y_dslr = data_dslr['labels'].flatten()\n",
    "\n",
    "# Regularization parameter\n",
    "rege = 0.01\n",
    "\n",
    "# Call the function\n",
    "accuracy = regularized_OT_classification(X_webcam, X_dslr, rege, y_dslr)\n",
    "print(f'Accuracy on target observations: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
